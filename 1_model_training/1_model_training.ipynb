{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import os\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import ast\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64784d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../0_dataset/dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcc06c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correlation matrix\n",
    "corr_matrix = X.corr().abs()\n",
    "corr_matrix\n",
    "\n",
    "# fueature selection\n",
    "# Drop features with correlation greater than 0.9\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
    "to_drop = set(to_drop)\n",
    "to_drop = [item for item in to_drop if item in df.columns]\n",
    "X = X.drop(columns=to_drop)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4594d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, LeaveOneOut\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# Linear Regression \n",
    "linear_params = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False]\n",
    "}\n",
    "\n",
    "# Ridge \n",
    "ridge_params = {\n",
    "    'alpha': (0.005, 10.0),\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False],\n",
    "    'max_iter': (100, 500),\n",
    "    'tol': (1e-5, 1e-0),\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n",
    "\n",
    "}\n",
    "\n",
    "# Decision Tree\n",
    "# decision_tree_params = {\n",
    "#     'criterion': ['absolute_error', 'squared_error', 'friedman_mse'],\n",
    "#     'splitter': ['best', 'random'],\n",
    "#     'max_depth': (1, 30),\n",
    "#     'min_samples_split': (2, 20),\n",
    "#     'min_samples_leaf': (1, 20),\n",
    "#     'max_features': ['sqrt', 'log2'],\n",
    "#     'min_impurity_decrease': (0, 0.5),\n",
    "#     'random_state': (0, 100)\n",
    "# }\n",
    "\n",
    "# # Extra Tree \n",
    "# extra_tree_params = {\n",
    "#     'criterion': ['absolute_error', 'squared_error', 'friedman_mse'],\n",
    "#     'splitter': ['best', 'random'],\n",
    "#     'max_depth': (1, 30),\n",
    "#     'min_samples_split': (2, 10),\n",
    "#     'min_samples_leaf': (1, 10),\n",
    "#     'min_weight_fraction_leaf': (0, 0.5),\n",
    "#     'max_features': ['sqrt', 'log2'],\n",
    "#     'min_impurity_decrease': (0, 0.5),\n",
    "#     #'warm_start': [True, False],\n",
    "#     'random_state': (0, 100),\n",
    "#     'ccp_alpha': (0, 1.0)\n",
    "# }\n",
    "\n",
    "# Random Forest \n",
    "random_forest_params = {\n",
    "    'n_estimators': (10, 500),\n",
    "    'criterion': ['absolute_error', 'squared_error', 'friedman_mse'],\n",
    "    'max_depth': (1, 100),\n",
    "    'min_samples_split': (2, 100),\n",
    "    'min_samples_leaf': (1, 100),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'random_state': (0, 100)\n",
    "}\n",
    "\n",
    "# Ada Boost \n",
    "ada_boost_params = {\n",
    "    'n_estimators': (10, 1000), \n",
    "    'learning_rate': (0.005, 1.0),\n",
    "    'random_state': (0, 100),\n",
    "    'loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "# Gradient Boosting \n",
    "gradient_boosting_params = {\n",
    "    'n_estimators': (100, 500),\n",
    "    'learning_rate': (0.005, 1.0),\n",
    "    'loss': ['huber', 'squared_error', 'quantile', 'absolute_error'],\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'max_depth': (1, 100),\n",
    "    'min_samples_split': (2, 100),\n",
    "    'min_samples_leaf': (1, 100),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'n_iter_no_change': (20, 50),\n",
    "    'random_state': (0, 100),\n",
    "    'criterion': ['squared_error', 'friedman_mse']\n",
    "}\n",
    "\n",
    "# LGBM \n",
    "lgbm_params = {\n",
    "    'n_estimators': (100, 500),\n",
    "    'learning_rate': (0.01, 1.0),\n",
    "    'num_leaves': (10, 100),\n",
    "    'max_depth': (1, 100),\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0),\n",
    "    'min_child_samples': (1, 10),\n",
    "    'min_child_weight': (0.001, 0.1),\n",
    "    'reg_alpha': (0.0, 1.0),\n",
    "    'reg_lambda': (0.0, 1.0),\n",
    "    'learning_rate': (0.01, 1.0),\n",
    "    'n_estimators': (100, 200),\n",
    "    'max_depth': (2, 10),\n",
    "    'num_leaves': (10, 30),\n",
    "    'min_data_in_leaf': (10, 30),\n",
    "    'lambda_l1': (0.1, 10.0),\n",
    "    'lambda_l2': (0.1, 10.0),\n",
    "    'bagging_fraction': (0.5, 1.0),\n",
    "    'bagging_freq': (1, 2, 3),\n",
    "    'feature_fraction': (0.5, 1.0),\n",
    "    'random_state': (0, 100)\n",
    "}\n",
    "\n",
    "# XGB \n",
    "xgb_params = {\n",
    "    'learning_rate': (0.01, 1.0),\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3, 50),\n",
    "    'min_child_weight': (1, 3),\n",
    "    'gamma': (0.1, 10.0),\n",
    "    'subsample': (0.5, 1.0),\n",
    "    'colsample_bytree': (0.5, 1.0),\n",
    "    'lambda': (0.1, 10.0),\n",
    "    'alpha': (0.1, 10.0),\n",
    "    'random_state': (0, 100)\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [\n",
    "    ('Gradient Boosting Regressor', GradientBoostingRegressor(), gradient_boosting_params),\n",
    "    ('Random Forest Regressor', RandomForestRegressor(), random_forest_params),\n",
    "    ('Ada Boost Regressor', AdaBoostRegressor(), ada_boost_params),\n",
    "    ('Linear Regression', LinearRegression(), linear_params),\n",
    "    # ('K Neighbors Regressor', KNeighborsRegressor()),\n",
    "    # ('SVR', SVR(C=1000,gamma=0.01, kernel='rbf')),#C=1000000\n",
    "    ('Ridge', Ridge(),ridge_params),\n",
    "    #('Lasso', Lasso(alpha=0.1, max_iter=100,random_state=42)),\n",
    "    # ('MLP Regressor', MLPRegressor(hidden_layer_sizes=(160,80), activation='relu', solver='lbfgs', learning_rate_init=0.001, max_iter=1000, random_state=42)),\n",
    "    # ('Decision Tree Regressor', DecisionTreeRegressor(random_state=42), decision_tree_params),\n",
    "    # ('Extra Tree Regressor', ExtraTreeRegressor(random_state=42), extra_tree_params),\n",
    "    #('Bagging Regressor', BaggingRegressor(estimator=RandomForestRegressor(), random_state=42))\n",
    "    ('LGBM Regressor', LGBMRegressor(), lgbm_params),\n",
    "    ('XGB Regressor', XGBRegressor(), xgb_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {}\n",
    "\n",
    "results = {}\n",
    "feature_results = {}\n",
    "cv_results = {}\n",
    "\n",
    "\n",
    "for name, regressor, params in regressors:\n",
    "\n",
    "    if name in ['Decision Tree Regressor', 'Extra Tree Regressor']:\n",
    "        step = 0.5\n",
    "        min_features_to_select = 5\n",
    "    elif name in [\"Ridge\"]:\n",
    "        step = 1\n",
    "        min_features_to_select = 5\n",
    "    elif name in [\"Ada Boost Regressor\"]:\n",
    "        step = 0.3\n",
    "        min_features_to_select = 4\n",
    "    elif name in [\"Gradient Boosting Regressor\"]:\n",
    "        step = 0.30\n",
    "        min_features_to_select = 4\n",
    "    elif name in [ \"LGBM Regressor\"]:\n",
    "        step = 0.3\n",
    "        min_features_to_select = 5\n",
    "    elif name in ['XGB Regressor']:\n",
    "        step = 0.3\n",
    "        min_features_to_select = 5\n",
    "    elif name in ['Random Forest Regressor']:\n",
    "        step = 0.3\n",
    "        min_features_to_select = 4\n",
    "    else: \n",
    "        step = 0.2\n",
    "        min_features_to_select = 5\n",
    "\n",
    "\n",
    "    selector = RFECV(regressor, step=step, cv=10, min_features_to_select=min_features_to_select)\n",
    "    X_wrapper = selector.fit_transform(X, y)\n",
    "    \n",
    "\n",
    "    # \n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=)\n",
    "    train_x_w, test_x_w, train_y, test_y = train_test_split(X_wrapper, y, test_size=0.1, random_state=)\n",
    "    # \n",
    "    scaler = StandardScaler()\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "    train_x_w = scaler.fit_transform(train_x_w)\n",
    "    test_x_w = scaler.transform(test_x_w)\n",
    "    \n",
    "    print(\"Regressor is %s\" % name)\n",
    "    print(\"N_features %s\" % selector.n_features_)\n",
    "    print(\"Support is %s\" % selector.support_)\n",
    "    print(\"Ranking %s\" % selector.ranking_)\n",
    "\n",
    "\n",
    "    feature_results[name] = {\n",
    "        'n_features': selector.n_features_,\n",
    "        'support': selector.support_,\n",
    "        'ranking': selector.ranking_\n",
    "    }\n",
    "    \n",
    "\n",
    "    for i in feature_results:\n",
    "        idx_list = []\n",
    "        for idx, item in enumerate(feature_results[i][\"support\"]):\n",
    "            if item == True:\n",
    "                idx_list.append(idx)\n",
    "\n",
    "    feature_names = []\n",
    "    for i in idx_list:\n",
    "        feature_names.append(X.columns[i])\n",
    "    print(\"Feature names is %s\" % feature_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    search_space = params\n",
    "\n",
    "    cv = 10\n",
    "\n",
    "    search = BayesSearchCV(estimator=regressor, search_spaces=search_space, cv=cv, n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    search.fit(train_x_w, train_y)\n",
    "    model_w = regressor.set_params(**search.best_params_)\n",
    "    print(model_w.get_params())\n",
    "\n",
    "    model_params[name] =  model_w.get_params()\n",
    "\n",
    "\n",
    "    model_w.fit(train_x_w, train_y)\n",
    "    pred_y_train_w = model_w.predict(train_x_w)\n",
    "    pred_y_test_w = model_w.predict(test_x_w)\n",
    "\n",
    "    train_r2_w = r2_score(train_y, pred_y_train_w)\n",
    "    test_r2_w = r2_score(test_y, pred_y_test_w)\n",
    "    rmse_train_w = np.sqrt(mean_absolute_error(train_y, pred_y_train_w))\n",
    "    rmse_test_w = np.sqrt(mean_absolute_error(test_y, pred_y_test_w))\n",
    "\n",
    "\n",
    "    cv_scores_w = cross_val_score(model_w, train_x_w, train_y, cv=10, scoring='neg_mean_absolute_error')\n",
    "    cv_rmse_w = np.sqrt(-cross_val_score(model_w, train_x_w, train_y, cv=10, scoring='neg_mean_squared_error'))\n",
    "    r2_scores_w = cross_val_score(model_w, train_x_w, train_y, cv=10, scoring='r2')\n",
    "    avg_mae_w = -np.mean(cv_scores_w)\n",
    "    avg_rmse_w = np.mean(cv_rmse_w)\n",
    "    avg_r2_w = np.mean(r2_scores_w)\n",
    "\n",
    "    loo_predicted_values = cross_val_predict(model_w, train_x_w, train_y.values, cv=LeaveOneOut())\n",
    "    loo_mae = cross_val_score(model_w, train_x_w, train_y.values, cv=LeaveOneOut(), scoring='neg_mean_squared_error')\n",
    "    loo_rmse = np.sqrt(-loo_mae)\n",
    "    loo_r2 = cross_val_score(model_w, train_x_w, train_y.values, cv=LeaveOneOut(), scoring='r2')\n",
    "    loo_predicted_series = pd.Series(loo_predicted_values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if name in [\"Ridge\", \"Linear Regression\"]:\n",
    "        feature_importance = model_w.coef_\n",
    "    else:\n",
    "        feature_importance = model_w.feature_importances_\n",
    "    \n",
    "\n",
    "    results[name] = {\n",
    "        'train_r2_w': float(f\"{train_r2_w:.4f}\"), 'test_r2_w': float(f\"{test_r2_w:.4f}\"), \n",
    "        'rmse_train_w': float(f\"{rmse_train_w:.4f}\"), 'rmse_test_w': float(f\"{rmse_test_w:.4f}\"),\n",
    "        'mae_train_w': float(f\"{mean_absolute_error(train_y, pred_y_train_w):.4f}\"), 'mae_test_w': float(f\"{mean_absolute_error(test_y, pred_y_test_w):.4f}\"),\n",
    "        'cv_avg_mae_w': float(f\"{avg_mae_w:.4f}\"), 'cv_avg_rmse_w': float(f\"{avg_rmse_w:.4f}\"), 'cv_avg_r2_w': float(f\"{avg_r2_w:.4f}\"),\n",
    "        'loo_mae': float(f\"{mean_absolute_error(train_y, loo_predicted_values):.4f}\"), 'loo_r2': float(f\"{r2_score(train_y, loo_predicted_values):.4f}\"),\n",
    "        \"n_features\": int(selector.n_features_),\n",
    "        'model_params': model_w.get_params(),\n",
    "        'feature_importance': feature_importance.tolist(),\n",
    "        'feature_names': feature_names\n",
    "    }\n",
    "\n",
    "\n",
    "    print('-------------------------------------------------------------')\n",
    "    print(name)\n",
    "    print(\"\\nSelected input:\")\n",
    "    print(f\"Train r2_w: {train_r2_w:.4f}, Test r2_w: {test_r2_w:.4f}\")\n",
    "    print(f\"Train RMSE_w: {rmse_train_w:.4f}, Test RMSE_w: {rmse_test_w:.4f}\")\n",
    "    print(\"\\nCross validation:\")\n",
    "    print(f\"CV avg MAE_w: {avg_mae_w:.4f}, CV avg RMSE_w: {avg_rmse_w:.4f}, CV avg R2_w: {avg_r2_w:.4f}\")\n",
    "    print('MAE_LOOCV:', mean_absolute_error(train_y, loo_predicted_values))\n",
    "    print('R2_LOOCV:', r2_score(train_y, loo_predicted_values))\n",
    "    print('-------------------------------------------------------------')\n",
    "    print('loo_mae:', loo_mae)\n",
    "    print('loo_rmse:', loo_rmse)\n",
    "    print('loo_r2:', loo_r2)\n",
    "    print(\"cv_mae:\", cv_scores_w)\n",
    "    print('cv_rmse_w:', cv_rmse_w)\n",
    "    print('r2_scores_w:', r2_scores_w)\n",
    "    print('-------------------------------------------------------------')\n",
    "        \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "    ax.bar(x=range(len(feature_importance)), height=feature_importance, tick_label=feature_names, color=\"#1d91c0\")\n",
    "\n",
    "    ax.set_ylabel('Feature importance')\n",
    "    ax.bar(x=range(len(feature_importance)), height=feature_importance, tick_label=feature_names)\n",
    "    plt.savefig(f'{name}_feature_importance.svg', dpi=600, format='svg')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "    plt.rc('font',family='Calibri', size=22)\n",
    "    \n",
    "    train = ax.scatter(train_y, pred_y_train_w, s=100, c='blue')\n",
    "    test = ax.scatter(test_y, pred_y_test_w, s=100, c='red')\n",
    "    #val = ax.scatter(val_y, pred_y_val, s=100, c='pink')\n",
    "    ax.plot([-1.5, 0.5],[-1.5, 0.5], c='black', linewidth=3.5)\n",
    "    ax.plot([-1.5, 0.4], [-1.4, 0.5], c='black', linewidth=2, linestyle='--')\n",
    "    ax.plot([-1.4, 0.5], [-1.5, 0.4], c='black', linewidth=2, linestyle='--')\n",
    "\n",
    "    ax.legend((train, test), ('Training set', 'Test set'), loc='lower right')\n",
    "    ax.set_xlim(-1.5, 0.5)\n",
    "    ax.set_ylim(-1.5, 0.5)\n",
    "    ax.set_xlabel('ΔΔG / (kcal/mol)')\n",
    "    ax.set_ylabel('Predicted ΔΔG / (kcal/mol)')\n",
    "    ax.text(x=-1.45, y=-0.2,\n",
    "        # s=f\"{name}\\nTrain r2: {train_r2:.4f}\\nTest r2: {test_r2:.4f}\\nTrain RMSE: {rmse_train:.4f}\\nTest RMSE: {rmse_test:.4f}\\nMAE_LOOCV: {mean_absolute_error(train_y, loo_predicted_values):.4f}\\nR2_LOOCV: {r2_score(train_y, loo_predicted_values):.4f}\")\n",
    "        # s = \"{name}\\nTrain r²: {train_r2:.4f}\\nTest r²: {test_r2:.4f}\\nTrain RMSE: {rmse_train:.4f}\\nTest RMSE: {rmse_test:.4f}\\nMAE_{LOOCV}: {mean_absolute_error(train_y, loo_predicted_values):.4f}\\nR²$_{LOOCV}$: {r2_score(train_y, loo_predicted_values):.4f}\"\n",
    "        s = '\\n{}\\nTrain $R^2$ = {:.4f} \\n Test $R^2$ = {:.4f}\\n$R^2_L$ = {:.4f}\\nTrain MAE = {:.4f}\\nTest MAE = {:.4f}\\nLOO MAE = {:.4f}'.format(name, train_r2_w, test_r2_w, r2_score(train_y, loo_predicted_values), mean_absolute_error(train_y, pred_y_train_w), mean_absolute_error(test_y, pred_y_test_w),mean_absolute_error(train_y, loo_predicted_values)),\n",
    "        fontsize=22\n",
    "    )\n",
    "    plt.savefig(f'{name}.svg', dpi=600, format='svg')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(results)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
